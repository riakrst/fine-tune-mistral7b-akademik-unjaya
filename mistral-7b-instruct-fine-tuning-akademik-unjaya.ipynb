{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BUSINESS UNDERSTANDING\n",
    "\n",
    "Tujuan Bisnis:\n",
    "- Menyediakan fondasi awal untuk pengembangan chatbot akademik berbasis LLM melalui proses fine-tuning model menggunakan data dari pedoman akademik Unjaya.\n",
    "- Meningkatkan aksesibilitas dan pemahaman terhadap informasi akademik di lingkungan Universitas Jenderal Achmad Yani Yogyakarta melalui inovasi teknologi AI.\n",
    "- Mendukung efisiensi penyampaian informasi dengan mengurangi beban kerja staf akademik dalam menjawab pertanyaan berulang seputar pedoman akademik.\n",
    "- Evaluasi menggunakan BERTScore "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persiapan \n",
    "\n",
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:42:57.960562Z",
     "iopub.status.busy": "2025-07-17T02:42:57.960287Z",
     "iopub.status.idle": "2025-07-17T02:42:57.967679Z",
     "shell.execute_reply": "2025-07-17T02:42:57.966691Z",
     "shell.execute_reply.started": "2025-07-17T02:42:57.960540Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tf5UrSC902WA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Menginstal library \n",
    "# !pip install peft datasets transformers trl accelerate bitsandbytes evaluate wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U bert_score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:14.865502Z",
     "iopub.status.busy": "2025-07-17T02:43:14.864881Z",
     "iopub.status.idle": "2025-07-17T02:43:33.489056Z",
     "shell.execute_reply": "2025-07-17T02:43:33.488194Z",
     "shell.execute_reply.started": "2025-07-17T02:43:14.865474Z"
    },
    "id": "I2Q2QOHb0_xo",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 02:43:26.177808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752720206.202460     260 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752720206.209891     260 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer #gak bisa di install\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling #alternative trainer\n",
    "\n",
    "import evaluate # Mengimpor library evaluate untuk BERTScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:33.491170Z",
     "iopub.status.busy": "2025-07-17T02:43:33.490407Z",
     "iopub.status.idle": "2025-07-17T02:43:33.495507Z",
     "shell.execute_reply": "2025-07-17T02:43:33.494705Z",
     "shell.execute_reply.started": "2025-07-17T02:43:33.491149Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 2.2.3\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numpy version: {pd.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:33.496619Z",
     "iopub.status.busy": "2025-07-17T02:43:33.496318Z",
     "iopub.status.idle": "2025-07-17T02:43:33.516848Z",
     "shell.execute_reply": "2025-07-17T02:43:33.516181Z",
     "shell.execute_reply.started": "2025-07-17T02:43:33.496594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pengaturan logging\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAHBndJW1C8C"
   },
   "source": [
    "### Login Huggingface dan Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:33.518660Z",
     "iopub.status.busy": "2025-07-17T02:43:33.518438Z",
     "iopub.status.idle": "2025-07-17T02:43:33.785505Z",
     "shell.execute_reply": "2025-07-17T02:43:33.784872Z",
     "shell.execute_reply.started": "2025-07-17T02:43:33.518644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Autentikasi dan setup\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "secret_wandb = user_secrets.get_secret(\"WANDB_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:33.786558Z",
     "iopub.status.busy": "2025-07-17T02:43:33.786267Z",
     "iopub.status.idle": "2025-07-17T02:43:34.467408Z",
     "shell.execute_reply": "2025-07-17T02:43:34.466492Z",
     "shell.execute_reply.started": "2025-07-17T02:43:33.786528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `kaggle_llm` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `kaggle_llm`\n"
     ]
    }
   ],
   "source": [
    "# Login ke Hugging Face Hub\n",
    "!huggingface-cli login --token $secret_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:34.468818Z",
     "iopub.status.busy": "2025-07-17T02:43:34.468518Z",
     "iopub.status.idle": "2025-07-17T02:43:48.959547Z",
     "shell.execute_reply": "2025-07-17T02:43:48.958682Z",
     "shell.execute_reply.started": "2025-07-17T02:43:34.468775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mriakrst\u001b[0m (\u001b[33mriakrst-universitas-jenderal-achmad-yani-yogyakarta\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250717_024340-8s24e2f5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/riakrst-universitas-jenderal-achmad-yani-yogyakarta/Fine-tuning-Mistral-7B-Pedoman-Akademik/runs/8s24e2f5' target=\"_blank\">worldly-leaf-6</a></strong> to <a href='https://wandb.ai/riakrst-universitas-jenderal-achmad-yani-yogyakarta/Fine-tuning-Mistral-7B-Pedoman-Akademik' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/riakrst-universitas-jenderal-achmad-yani-yogyakarta/Fine-tuning-Mistral-7B-Pedoman-Akademik' target=\"_blank\">https://wandb.ai/riakrst-universitas-jenderal-achmad-yani-yogyakarta/Fine-tuning-Mistral-7B-Pedoman-Akademik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/riakrst-universitas-jenderal-achmad-yani-yogyakarta/Fine-tuning-Mistral-7B-Pedoman-Akademik/runs/8s24e2f5' target=\"_blank\">https://wandb.ai/riakrst-universitas-jenderal-achmad-yani-yogyakarta/Fine-tuning-Mistral-7B-Pedoman-Akademik/runs/8s24e2f5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login ke Weights & Biases untuk monitoring pelatihan\n",
    "wandb.login(key = secret_wandb)\n",
    "run = wandb.init(\n",
    "    project='Fine-tuning-Mistral-7B-Pedoman-Akademik',\n",
    "    job_type=\"training\",\n",
    "    notes=\"Fine-tuning Mistral 7B Instruct v0.3 untuk chatbot pedoman akademik\",\n",
    "    tags=[\"mistral\", \"chatbot\", \"academic\", \"qlora\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbWS_WAk1SdJ"
   },
   "source": [
    "# 2. DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:48.960869Z",
     "iopub.status.busy": "2025-07-17T02:43:48.960566Z",
     "iopub.status.idle": "2025-07-17T02:43:49.008721Z",
     "shell.execute_reply": "2025-07-17T02:43:49.008110Z",
     "shell.execute_reply.started": "2025-07-17T02:43:48.960839Z"
    },
    "id": "RWP6R8GD1WXD",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Response</th>\n",
       "      <th>Sumber</th>\n",
       "      <th>ID_Original</th>\n",
       "      <th>Tipe_Variasi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Apa itu pedoman akademik di Unjaya?</td>\n",
       "      <td>Pedoman akademik adalah jabaran dari kebijakan...</td>\n",
       "      <td>Kata Pengantar, Hal. 3 (Pedoman Akademik Unjay...</td>\n",
       "      <td>1</td>\n",
       "      <td>Original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Apa tujuan dari penyusunan pedoman akademik Un...</td>\n",
       "      <td>Tujuannya adalah menjadi panduan menyeluruh ba...</td>\n",
       "      <td>Kata Pengantar, Hal. 3 (Pedoman Akademik Unjay...</td>\n",
       "      <td>2</td>\n",
       "      <td>Original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Apa saja yang dicakup dalam pedoman akademik U...</td>\n",
       "      <td>Pedoman mencakup kebijakan mutu, visi, misi, t...</td>\n",
       "      <td>Kata Pengantar, Hal. 3 (Pedoman Akademik Unjay...</td>\n",
       "      <td>3</td>\n",
       "      <td>Original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Siapa yang menyusun pedoman akademik Unjaya 2024?</td>\n",
       "      <td>Tim penyusun terdiri dari Niko Wahyu Nurcahyo,...</td>\n",
       "      <td>Kata Pengantar, Hal. 4 (Pedoman Akademik Unjay...</td>\n",
       "      <td>4</td>\n",
       "      <td>Original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Siapa Rektor Universitas Jenderal Achmad Yani ...</td>\n",
       "      <td>Rektor Unjaya adalah Prof. Dr.rer.nat.apt. Tri...</td>\n",
       "      <td>Struktur Organisasi Universitas Jenderal Achma...</td>\n",
       "      <td>5</td>\n",
       "      <td>Original</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                        Instruction  \\\n",
       "0   1                Apa itu pedoman akademik di Unjaya?   \n",
       "1   2  Apa tujuan dari penyusunan pedoman akademik Un...   \n",
       "2   3  Apa saja yang dicakup dalam pedoman akademik U...   \n",
       "3   4  Siapa yang menyusun pedoman akademik Unjaya 2024?   \n",
       "4   5  Siapa Rektor Universitas Jenderal Achmad Yani ...   \n",
       "\n",
       "                                            Response  \\\n",
       "0  Pedoman akademik adalah jabaran dari kebijakan...   \n",
       "1  Tujuannya adalah menjadi panduan menyeluruh ba...   \n",
       "2  Pedoman mencakup kebijakan mutu, visi, misi, t...   \n",
       "3  Tim penyusun terdiri dari Niko Wahyu Nurcahyo,...   \n",
       "4  Rektor Unjaya adalah Prof. Dr.rer.nat.apt. Tri...   \n",
       "\n",
       "                                              Sumber  ID_Original Tipe_Variasi  \n",
       "0  Kata Pengantar, Hal. 3 (Pedoman Akademik Unjay...            1     Original  \n",
       "1  Kata Pengantar, Hal. 3 (Pedoman Akademik Unjay...            2     Original  \n",
       "2  Kata Pengantar, Hal. 3 (Pedoman Akademik Unjay...            3     Original  \n",
       "3  Kata Pengantar, Hal. 4 (Pedoman Akademik Unjay...            4     Original  \n",
       "4  Struktur Organisasi Universitas Jenderal Achma...            5     Original  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memuat dataset dari file CSV\n",
    "df = pd.read_csv('/kaggle/input/dataset-pedoman-akademik-unjaya/Dataset Pedoman Akademik 2024.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:49.009670Z",
     "iopub.status.busy": "2025-07-17T02:43:49.009454Z",
     "iopub.status.idle": "2025-07-17T02:43:49.022895Z",
     "shell.execute_reply": "2025-07-17T02:43:49.022044Z",
     "shell.execute_reply.started": "2025-07-17T02:43:49.009653Z"
    },
    "id": "VR5kt_3H1aTu",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1626 entries, 0 to 1625\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   ID            1626 non-null   int64 \n",
      " 1   Instruction   1626 non-null   object\n",
      " 2   Response      1626 non-null   object\n",
      " 3   Sumber        1626 non-null   object\n",
      " 4   ID_Original   1626 non-null   int64 \n",
      " 5   Tipe_Variasi  1626 non-null   object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 76.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:49.023898Z",
     "iopub.status.busy": "2025-07-17T02:43:49.023673Z",
     "iopub.status.idle": "2025-07-17T02:43:49.045011Z",
     "shell.execute_reply": "2025-07-17T02:43:49.044329Z",
     "shell.execute_reply.started": "2025-07-17T02:43:49.023880Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Original', 'gaya_pertanyaan'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tipe_Variasi'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0fwe_Jl1p7r"
   },
   "source": [
    "# 3. DATA PREPARATION\n",
    "## 3.1. Split Train dan Eval Set Secara Adil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:49.049254Z",
     "iopub.status.busy": "2025-07-17T02:43:49.048612Z",
     "iopub.status.idle": "2025-07-17T02:43:49.081377Z",
     "shell.execute_reply": "2025-07-17T02:43:49.080468Z",
     "shell.execute_reply.started": "2025-07-17T02:43:49.049235Z"
    },
    "id": "zO7QmNBC1wCO",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data original   : 413\n",
      "Jumlah variasi - train : 1091\n",
      "Jumlah variasi - eval  : 122\n",
      "Jumlah total train     : 1504\n",
      "Jumlah total eval      : 122\n"
     ]
    }
   ],
   "source": [
    "# Cek kolom penting\n",
    "assert 'Instruction' in df.columns, \"Kolom 'Instruction' tidak ditemukan.\"\n",
    "assert 'ID' in df.columns, \"Kolom 'ID' tidak ditemukan.\"\n",
    "\n",
    "# Pisahkan data original (ID 1–413) dan data variasi (ID 414 ke atas)\n",
    "df_original = df[df['ID'] <= 413].reset_index(drop=True)\n",
    "df_variasi = df[df['ID'] > 413].reset_index(drop=True)\n",
    "\n",
    "# Binning untuk stratifikasi hanya pada data variasi (ID 414 ke atas)\n",
    "df_variasi['length_bin'] = pd.qcut(\n",
    "    df_variasi['Instruction'].str.len(), \n",
    "    q=5, \n",
    "    labels=False, \n",
    "    duplicates='drop'\n",
    ")\n",
    "\n",
    "# Stratified split hanya untuk data variasi\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "for train_idx, eval_idx in splitter.split(df_variasi, df_variasi['length_bin']):\n",
    "    df_variasi_train = df_variasi.iloc[train_idx].reset_index(drop=True)\n",
    "    df_variasi_eval = df_variasi.iloc[eval_idx].reset_index(drop=True)\n",
    "\n",
    "# Gabungkan kembali seluruh data original masuk ke train dan df_variasi_train\n",
    "df_train = pd.concat([df_original, df_variasi_train], ignore_index=True)\n",
    "df_eval = df_variasi_eval.copy()\n",
    "\n",
    "# Hapus kolom bantu\n",
    "df_train = df_train.drop(columns=['length_bin'], errors='ignore')\n",
    "df_eval = df_eval.drop(columns=['length_bin'], errors='ignore')\n",
    "\n",
    "# Validasi tidak ada null di kolom penting\n",
    "assert df_train['Instruction'].isnull().sum() == 0, \"Ada instruksi kosong di train\"\n",
    "assert df_eval['Instruction'].isnull().sum() == 0, \"Ada instruksi kosong di eval\"\n",
    "assert df_train['Response'].isnull().sum() == 0, \"Ada respons kosong di train\"\n",
    "assert df_eval['Response'].isnull().sum() == 0, \"Ada respons kosong di eval\"\n",
    "\n",
    "# Output hasil\n",
    "print(f\"Jumlah data original   : {len(df_original)}\")\n",
    "print(f\"Jumlah variasi - train : {len(df_variasi_train)}\")\n",
    "print(f\"Jumlah variasi - eval  : {len(df_variasi_eval)}\")\n",
    "print(f\"Jumlah total train     : {len(df_train)}\")\n",
    "print(f\"Jumlah total eval      : {len(df_eval)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:49.082489Z",
     "iopub.status.busy": "2025-07-17T02:43:49.082244Z",
     "iopub.status.idle": "2025-07-17T02:43:49.095706Z",
     "shell.execute_reply": "2025-07-17T02:43:49.094946Z",
     "shell.execute_reply.started": "2025-07-17T02:43:49.082471Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Response</th>\n",
       "      <th>Sumber</th>\n",
       "      <th>ID_Original</th>\n",
       "      <th>Tipe_Variasi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>622</td>\n",
       "      <td>Apa peran dan posisi Dekan menurut pedoman ini?</td>\n",
       "      <td>Dekan adalah pimpinan tertinggi fakultas dalam...</td>\n",
       "      <td>Bab II Ketentuan Umum, Pasal 2 Daftar Istilah ...</td>\n",
       "      <td>71</td>\n",
       "      <td>gaya_pertanyaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1366</td>\n",
       "      <td>Unjaya melibatkan siapa saja dalam proses peni...</td>\n",
       "      <td>Penilaian dapat dilakukan oleh: a) dosen penga...</td>\n",
       "      <td>Bab IX Penilaian Pembelajaran, Pasal 42 Pelaks...</td>\n",
       "      <td>329</td>\n",
       "      <td>gaya_pertanyaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>751</td>\n",
       "      <td>Bisa dijelaskan keputusan menteri yang dijadik...</td>\n",
       "      <td>(1) Keputusan Menteri Pendidikan dan Kebudayaa...</td>\n",
       "      <td>Bab III Dasar Hukum Pedoman, Pasal 3 Dasar Huk...</td>\n",
       "      <td>114</td>\n",
       "      <td>gaya_pertanyaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>854</td>\n",
       "      <td>Langkah apa saja yang harus dilakukan sebelum ...</td>\n",
       "      <td>\"Sebelum pengisian KRS, setiap mahasiswa wajib...</td>\n",
       "      <td>Bab V Administrasi dan Registrasi Akademik, Pa...</td>\n",
       "      <td>149</td>\n",
       "      <td>gaya_pertanyaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>614</td>\n",
       "      <td>Apa tugas dan posisi Wakil Rektor I dalam pedo...</td>\n",
       "      <td>Wakil Rektor I (Warek I) adalah unsur pimpinan...</td>\n",
       "      <td>Bab II Ketentuan Umum, Pasal 2 Daftar Istilah ...</td>\n",
       "      <td>68</td>\n",
       "      <td>gaya_pertanyaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>657</td>\n",
       "      <td>Wisuda itu sebenarnya acara apa?</td>\n",
       "      <td>Wisuda adalah upacara akademik dalam forum rap...</td>\n",
       "      <td>Bab II Ketentuan Umum, Pasal 2 Daftar Istilah ...</td>\n",
       "      <td>83</td>\n",
       "      <td>gaya_pertanyaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>941</td>\n",
       "      <td>Siapa yang menerima laporan rutin dari DPA ter...</td>\n",
       "      <td>Setiap DPA berkewajiban melaporkan kegiatan bi...</td>\n",
       "      <td>Bab V Administrasi dan Registrasi Akademik, Pa...</td>\n",
       "      <td>179</td>\n",
       "      <td>gaya_pertanyaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>790</td>\n",
       "      <td>Unjaya menyediakan jenis RPL apa saja untuk la...</td>\n",
       "      <td>Jenis RPL untuk melanjutkan pendidikan formal ...</td>\n",
       "      <td>Bab IV Sistem Penerimaan dan Pendaftaran Mahas...</td>\n",
       "      <td>127</td>\n",
       "      <td>gaya_pertanyaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1521</td>\n",
       "      <td>Mahasiswa Unjaya yang sudah lulus yudisium, ap...</td>\n",
       "      <td>Ya, mahasiswa yang telah dinyatakan lulus dala...</td>\n",
       "      <td>Bab X Tugas Akhir, Yudisium, Wisuda, Pemberian...</td>\n",
       "      <td>379</td>\n",
       "      <td>gaya_pertanyaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1116</td>\n",
       "      <td>Seperti apa suasana belajar yang diterapkan sa...</td>\n",
       "      <td>Diselenggarakan dengan menciptakan suasana bel...</td>\n",
       "      <td>Bab VIII Sistem Pembelajaran, Pasal 31 Pelaksa...</td>\n",
       "      <td>245</td>\n",
       "      <td>gaya_pertanyaan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                        Instruction  \\\n",
       "0     622    Apa peran dan posisi Dekan menurut pedoman ini?   \n",
       "1    1366  Unjaya melibatkan siapa saja dalam proses peni...   \n",
       "2     751  Bisa dijelaskan keputusan menteri yang dijadik...   \n",
       "3     854  Langkah apa saja yang harus dilakukan sebelum ...   \n",
       "4     614  Apa tugas dan posisi Wakil Rektor I dalam pedo...   \n",
       "..    ...                                                ...   \n",
       "117   657                   Wisuda itu sebenarnya acara apa?   \n",
       "118   941  Siapa yang menerima laporan rutin dari DPA ter...   \n",
       "119   790  Unjaya menyediakan jenis RPL apa saja untuk la...   \n",
       "120  1521  Mahasiswa Unjaya yang sudah lulus yudisium, ap...   \n",
       "121  1116  Seperti apa suasana belajar yang diterapkan sa...   \n",
       "\n",
       "                                              Response  \\\n",
       "0    Dekan adalah pimpinan tertinggi fakultas dalam...   \n",
       "1    Penilaian dapat dilakukan oleh: a) dosen penga...   \n",
       "2    (1) Keputusan Menteri Pendidikan dan Kebudayaa...   \n",
       "3    \"Sebelum pengisian KRS, setiap mahasiswa wajib...   \n",
       "4    Wakil Rektor I (Warek I) adalah unsur pimpinan...   \n",
       "..                                                 ...   \n",
       "117  Wisuda adalah upacara akademik dalam forum rap...   \n",
       "118  Setiap DPA berkewajiban melaporkan kegiatan bi...   \n",
       "119  Jenis RPL untuk melanjutkan pendidikan formal ...   \n",
       "120  Ya, mahasiswa yang telah dinyatakan lulus dala...   \n",
       "121  Diselenggarakan dengan menciptakan suasana bel...   \n",
       "\n",
       "                                                Sumber  ID_Original  \\\n",
       "0    Bab II Ketentuan Umum, Pasal 2 Daftar Istilah ...           71   \n",
       "1    Bab IX Penilaian Pembelajaran, Pasal 42 Pelaks...          329   \n",
       "2    Bab III Dasar Hukum Pedoman, Pasal 3 Dasar Huk...          114   \n",
       "3    Bab V Administrasi dan Registrasi Akademik, Pa...          149   \n",
       "4    Bab II Ketentuan Umum, Pasal 2 Daftar Istilah ...           68   \n",
       "..                                                 ...          ...   \n",
       "117  Bab II Ketentuan Umum, Pasal 2 Daftar Istilah ...           83   \n",
       "118  Bab V Administrasi dan Registrasi Akademik, Pa...          179   \n",
       "119  Bab IV Sistem Penerimaan dan Pendaftaran Mahas...          127   \n",
       "120  Bab X Tugas Akhir, Yudisium, Wisuda, Pemberian...          379   \n",
       "121  Bab VIII Sistem Pembelajaran, Pasal 31 Pelaksa...          245   \n",
       "\n",
       "        Tipe_Variasi  \n",
       "0    gaya_pertanyaan  \n",
       "1    gaya_pertanyaan  \n",
       "2    gaya_pertanyaan  \n",
       "3    gaya_pertanyaan  \n",
       "4    gaya_pertanyaan  \n",
       "..               ...  \n",
       "117  gaya_pertanyaan  \n",
       "118  gaya_pertanyaan  \n",
       "119  gaya_pertanyaan  \n",
       "120  gaya_pertanyaan  \n",
       "121  gaya_pertanyaan  \n",
       "\n",
       "[122 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kv7bleC31zgH"
   },
   "source": [
    "## 3.2. Konversi ke Format Chat (ChatML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:49.097069Z",
     "iopub.status.busy": "2025-07-17T02:43:49.096764Z",
     "iopub.status.idle": "2025-07-17T02:43:49.219789Z",
     "shell.execute_reply": "2025-07-17T02:43:49.219075Z",
     "shell.execute_reply.started": "2025-07-17T02:43:49.097049Z"
    },
    "id": "f-jZnC3a15eV",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data disimpan di: /kaggle/working/train_chatml.jsonl\n",
      "Eval data disimpan di: /kaggle/working/eval_chatml.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk mengonversi DataFrame menjadi format JSONL yang sesuai dengan ChatML.\n",
    "# Setiap baris akan menjadi entri 'messages' dengan peran 'user' dan 'assistant'.\n",
    "def to_chatml_format(df, filename):\n",
    "    chat_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Format pesan untuk Mistral Instruct\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": row['Instruction']\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": f\"{row['Response']}\\n\\n(Sumber: {row['Sumber']})\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        chat_data.append({\"messages\": messages})\n",
    "    \n",
    "    # Simpan ke file JSONL\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for item in chat_data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    return chat_data\n",
    "\n",
    "# Konversi dan simpan data\n",
    "train_jsonl_path = '/kaggle/working/train_chatml.jsonl'\n",
    "eval_jsonl_path = '/kaggle/working/eval_chatml.jsonl'\n",
    "\n",
    "train_chatml = to_chatml_format(df_train, train_jsonl_path)\n",
    "eval_chatml = to_chatml_format(df_eval, eval_jsonl_path)\n",
    "\n",
    "print(f\"Train data disimpan di: {train_jsonl_path}\")\n",
    "print(f\"Eval data disimpan di: {eval_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:49.220832Z",
     "iopub.status.busy": "2025-07-17T02:43:49.220569Z",
     "iopub.status.idle": "2025-07-17T02:43:49.539814Z",
     "shell.execute_reply": "2025-07-17T02:43:49.538999Z",
     "shell.execute_reply.started": "2025-07-17T02:43:49.220791Z"
    },
    "id": "nsT5xWu92HqQ",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3385319982f47e59e8dba4824384775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4916f44e49cc4bbfba72e76e112ae3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset loaded: 1504 samples\n",
      "Eval dataset loaded: 122 samples\n"
     ]
    }
   ],
   "source": [
    "# Load dataset menggunakan Hugging Face datasets\n",
    "train_dataset = load_dataset('json', data_files=train_jsonl_path, split='train')\n",
    "eval_dataset = load_dataset('json', data_files=eval_jsonl_path, split='train')\n",
    "\n",
    "print(f\"Train dataset loaded: {len(train_dataset)} samples\")\n",
    "print(f\"Eval dataset loaded: {len(eval_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:49.540988Z",
     "iopub.status.busy": "2025-07-17T02:43:49.540639Z",
     "iopub.status.idle": "2025-07-17T02:43:49.548108Z",
     "shell.execute_reply": "2025-07-17T02:43:49.547294Z",
     "shell.execute_reply.started": "2025-07-17T02:43:49.540958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contoh format data ChatML:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Apa itu pedoman akademik di Unjaya?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Pedoman akademik adalah jabaran dari kebijakan akademik Universitas Jenderal Achmad Yani Yogyakarta yang menjadi pedoman penyelenggaraan program akademik.\\n\\n(Sumber: Kata Pengantar, Hal. 3 (Pedoman Akademik Unjaya 2024))\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Contoh format data\n",
    "print(\"\\nContoh format data ChatML:\")\n",
    "print(json.dumps(train_chatml[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybMd1AYE2KXq"
   },
   "source": [
    "\n",
    "# 4. MODELING\n",
    "\n",
    "## 4.1 Konfigurasi Model dan Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:43:49.549074Z",
     "iopub.status.busy": "2025-07-17T02:43:49.548843Z",
     "iopub.status.idle": "2025-07-17T02:44:18.607531Z",
     "shell.execute_reply": "2025-07-17T02:44:18.606779Z",
     "shell.execute_reply.started": "2025-07-17T02:43:49.549057Z"
    },
    "id": "KTdjVKbv2VTs",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/model.safetensors.index.json\n",
      "Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1651e26a92345babc17765c6eff9ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dan tokenizer berhasil dimuat\n",
      "Vocab size: 32768\n"
     ]
    }
   ],
   "source": [
    "# Konfigurasi QLoRA (4-bit quantization)\n",
    "# QLoRA memungkinkan fine-tuning model besar dengan memory GPU terbatas\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Kuantisasi model ke 4-bit (menghemat 75% memory)\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Normal Float 4: format kuantisasi yang optimal\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Tipe data untuk komputasi (lebih stabil dari float16)\n",
    "    bnb_4bit_use_double_quant=False      # Double quantization off (menghemat memory lebih)\n",
    ")\n",
    "# Model configuration\n",
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "new_model_name = \"riakrst/mistral-7b-pedoman-akademik-unjaya\"\n",
    "\n",
    "print(f\"Loading model: {base_model}\")\n",
    "\n",
    "# Load model dengan quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,      # Terapkan konfigurasi QLoRA\n",
    "    torch_dtype=torch.bfloat16,          # Tipe data model (lebih stabil dari float16)\n",
    "    device_map=\"auto\",                   # Otomatis distribusi ke GPU yang tersedia\n",
    "    trust_remote_code=True               # Izinkan eksekusi kode kustom dari model\n",
    ")\n",
    "\n",
    "# Disable cache untuk training (akan diaktifkan kembali untuk inference)\n",
    "model.config.use_cache = False          # Matikan cache untuk menghemat memory saat training\n",
    "model.config.pretraining_tp = 1         # Tensor parallelism = 1 (menghindari warning)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "# Setup tokenizer untuk chat format\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Gunakan end-of-sequence sebagai padding\n",
    "tokenizer.padding_side = \"right\"              # Padding di sebelah kanan (standar untuk causal LM)\n",
    "\n",
    "print(f\"Model dan tokenizer berhasil dimuat\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8LY-qr12X1D"
   },
   "source": [
    "## 4.2 Konfigurasi PEFT (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:44:18.609311Z",
     "iopub.status.busy": "2025-07-17T02:44:18.609060Z",
     "iopub.status.idle": "2025-07-17T02:44:19.878311Z",
     "shell.execute_reply": "2025-07-17T02:44:19.877499Z",
     "shell.execute_reply.started": "2025-07-17T02:44:18.609289Z"
    },
    "id": "61usOBbj2d0x",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 92,274,688 || all params: 7,340,298,240 || trainable%: 1.2571\n"
     ]
    }
   ],
   "source": [
    "# Persiapan model untuk kbit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Konfigurasi LoRA (Low-Rank Adaptation)\n",
    "# LoRA menambahkan layer kecil yang dapat dilatih tanpa mengubah model asli\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,                       # Skala untuk bobot LoRA (biasanya 16 atau 32)\n",
    "    lora_dropout=0.1,                    # Dropout untuk mencegah overfitting\n",
    "    r=64,                                # Rank matriks LoRA (semakin tinggi = lebih ekspresif)\n",
    "    bias=\"none\",                         # Tidak melatih bias (menghemat parameter)\n",
    "    task_type=\"CAUSAL_LM\",              # Tipe tugas: Causal Language Modeling\n",
    "    target_modules=[                     # Layer yang akan ditambahkan LoRA adapter\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"      # attention layers\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0zF9gKD3Azg"
   },
   "source": [
    "## 4.3 Konfigurasi SFT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:44:19.879349Z",
     "iopub.status.busy": "2025-07-17T02:44:19.879073Z",
     "iopub.status.idle": "2025-07-17T02:44:19.921524Z",
     "shell.execute_reply": "2025-07-17T02:44:19.920710Z",
     "shell.execute_reply.started": "2025-07-17T02:44:19.879324Z"
    },
    "id": "ePulV-Ov3Ftc",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir = \"/kaggle/working/results-pedoman-akademik\",\n",
    "    num_train_epochs=1,                         \n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    eval_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "\n",
    "    # Logging dan checkpointing\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10,\n",
    "    disable_tqdm=False,\n",
    "\n",
    "    # Evaluasi dan reproducibility\n",
    "    eval_strategy=\"steps\",                # pastikan evaluasi jalan\n",
    "    seed=42,\n",
    "\n",
    "    # Precision\n",
    "    bf16=False,                                 # T4 tidak support bf16, pakai fp16 otomatis\n",
    "    fp16=True,\n",
    "\n",
    "    # Hugging Face Hub\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=new_model_name,                \n",
    "\n",
    "    # Wandb monitoring \n",
    "    report_to=\"wandb\" if wandb.run else None,\n",
    "    run_name=f\"mistral-pedoman-{wandb.run.id}\" if wandb.run else None,\n",
    "\n",
    "    # SFT-specific params\n",
    "    max_seq_length=512,\n",
    "    packing=False,                              # False untuk dataset ChatML baris per dialog\n",
    "    neftune_noise_alpha=5,                      # Optional: bisa di-nol-kan juga\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:44:19.922656Z",
     "iopub.status.busy": "2025-07-17T02:44:19.922424Z",
     "iopub.status.idle": "2025-07-17T02:44:22.734739Z",
     "shell.execute_reply": "2025-07-17T02:44:22.733971Z",
     "shell.execute_reply.started": "2025-07-17T02:44:19.922637Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a535f630b3a44f98851aa1f48679dab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658026e388e54618bcf673e728566f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea966686a2654cf5920e22f7a188bb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5986b5993dd34a0f9a3f797c3fe485fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,              \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # tokenizer=tokenizer,          \n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T02:44:22.736402Z",
     "iopub.status.busy": "2025-07-17T02:44:22.735640Z",
     "iopub.status.idle": "2025-07-17T03:22:20.873157Z",
     "shell.execute_reply": "2025-07-17T03:22:20.872203Z",
     "shell.execute_reply.started": "2025-07-17T02:44:22.736382Z"
    },
    "id": "-YgOWB7D3Jc9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages. If messages are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1,504\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 47\n",
      "  Number of trainable parameters = 92,274,688\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 37:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.877800</td>\n",
       "      <td>1.592857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.383500</td>\n",
       "      <td>1.158748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.118700</td>\n",
       "      <td>1.009609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.997100</td>\n",
       "      <td>0.944186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages. If messages are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 122\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /kaggle/working/results-pedoman-akademik/checkpoint-10\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/checkpoint-10/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/checkpoint-10/special_tokens_map.json\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/special_tokens_map.json\n",
      "Deleting older checkpoint [/kaggle/working/results-pedoman-akademik/checkpoint-20] due to args.save_total_limit\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages. If messages are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 122\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /kaggle/working/results-pedoman-akademik/checkpoint-20\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/checkpoint-20/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/checkpoint-20/special_tokens_map.json\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/special_tokens_map.json\n",
      "Deleting older checkpoint [/kaggle/working/results-pedoman-akademik/checkpoint-10] due to args.save_total_limit\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages. If messages are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 122\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /kaggle/working/results-pedoman-akademik/checkpoint-30\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/checkpoint-30/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/checkpoint-30/special_tokens_map.json\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/special_tokens_map.json\n",
      "Deleting older checkpoint [/kaggle/working/results-pedoman-akademik/checkpoint-20] due to args.save_total_limit\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages. If messages are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 122\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /kaggle/working/results-pedoman-akademik/checkpoint-40\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/checkpoint-40/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/checkpoint-40/special_tokens_map.json\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/special_tokens_map.json\n",
      "Deleting older checkpoint [/kaggle/working/results-pedoman-akademik/checkpoint-30] due to args.save_total_limit\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Saving model checkpoint to /kaggle/working/results-pedoman-akademik/checkpoint-47\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/checkpoint-47/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/checkpoint-47/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/checkpoint-47/special_tokens_map.json\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/special_tokens_map.json\n",
      "Deleting older checkpoint [/kaggle/working/results-pedoman-akademik/checkpoint-40] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=47, training_loss=1.4998841691524425, metrics={'train_runtime': 2268.45, 'train_samples_per_second': 0.663, 'train_steps_per_second': 0.021, 'total_flos': 1.2234108777086976e+16, 'train_loss': 1.4998841691524425})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hindari error pembuatan model card\n",
    "trainer.create_model_card = lambda *args, **kwargs: None\n",
    "\n",
    "print(\"Memulai fine-tuning...\")\n",
    "\n",
    "# Log hyperparameters ke W&B\n",
    "if wandb.run:\n",
    "    wandb.config.update({\n",
    "        \"base_model\": base_model,\n",
    "        \"dataset_size\": len(train_dataset),\n",
    "        \"eval_size\": len(eval_dataset),\n",
    "        \"lora_r\": peft_config.r,\n",
    "        \"lora_alpha\": peft_config.lora_alpha,\n",
    "        \"learning_rate\": sft_config.learning_rate,  \n",
    "        \"batch_size\": sft_config.per_device_train_batch_size,  \n",
    "        \"epochs\": sft_config.num_train_epochs,  \n",
    "        \"max_seq_length\": sft_config.max_seq_length,\n",
    "        \"model_name\": new_model_name,\n",
    "    })\n",
    "# Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T03:22:20.874563Z",
     "iopub.status.busy": "2025-07-17T03:22:20.874235Z",
     "iopub.status.idle": "2025-07-17T03:23:18.197451Z",
     "shell.execute_reply": "2025-07-17T03:23:18.196690Z",
     "shell.execute_reply.started": "2025-07-17T03:22:20.874531Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages. If messages are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 122\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/61 00:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9180750846862793,\n",
       " 'eval_runtime': 57.3104,\n",
       " 'eval_samples_per_second': 2.129,\n",
       " 'eval_steps_per_second': 1.064}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GobkGbKi3VLY"
   },
   "source": [
    "# 5. EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T03:44:15.645901Z",
     "iopub.status.busy": "2025-07-17T03:44:15.645251Z",
     "iopub.status.idle": "2025-07-17T03:49:32.977540Z",
     "shell.execute_reply": "2025-07-17T03:49:32.976549Z",
     "shell.execute_reply.started": "2025-07-17T03:44:15.645873Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 15 responses...\n",
      "Progress: 1/15\n",
      "Progress: 2/15\n",
      "Progress: 3/15\n",
      "Progress: 4/15\n",
      "Progress: 5/15\n",
      "Progress: 6/15\n",
      "Progress: 7/15\n",
      "Progress: 8/15\n",
      "Progress: 9/15\n",
      "Progress: 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 11/15\n",
      "Progress: 12/15\n",
      "Progress: 13/15\n",
      "Progress: 14/15\n",
      "Progress: 15/15\n"
     ]
    }
   ],
   "source": [
    "# Set model ke mode eval\n",
    "model.config.use_cache = True\n",
    "model.eval()\n",
    "\n",
    "# Load BERTScore\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Sampling data evaluasi\n",
    "sample_size = min(15, len(eval_dataset))\n",
    "sample_data = eval_dataset.shuffle(seed=42).select(range(sample_size))\n",
    "\n",
    "# Ambil data\n",
    "prompts = [ex[\"messages\"][0][\"content\"] for ex in sample_data] # pesan dari user\n",
    "references = [ex[\"messages\"][1][\"content\"] for ex in sample_data] # pesan dari assistant\n",
    "\n",
    "# Setup pipeline (tanpa parameter device)\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Generate jawaban\n",
    "print(f\"Generating {sample_size} responses...\")\n",
    "generated = []\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Progress: {i+1}/{sample_size}\")\n",
    "    \n",
    "    try:\n",
    "        chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        input_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        output = pipe(\n",
    "            input_text,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        generated.append(output[0][\"generated_text\"].strip())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error pada sample {i+1}: {e}\")\n",
    "        generated.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T04:22:56.293992Z",
     "iopub.status.busy": "2025-07-17T04:22:56.293381Z",
     "iopub.status.idle": "2025-07-17T04:22:56.705130Z",
     "shell.execute_reply": "2025-07-17T04:22:56.704226Z",
     "shell.execute_reply.started": "2025-07-17T04:22:56.293960Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
      "    ColabKernelApp.launch_instance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3148, in run_cell_async\n",
      "    self.events.trigger('pre_run_cell', info)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/events.py\", line 89, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 575, in _pre_run_cell_hook\n",
      "    if self.notebook and self.notebook.save_ipynb():\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/jupyter.py\", line 404, in save_ipynb\n",
      "    logger.info(\"not saving jupyter notebook\")\n",
      "Message: 'not saving jupyter notebook'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
      "    ColabKernelApp.launch_instance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3148, in run_cell_async\n",
      "    self.events.trigger('pre_run_cell', info)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/events.py\", line 89, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 581, in _pre_run_cell_hook\n",
      "    self._logger.info(\"pausing backend\")\n",
      "Message: 'pausing backend'\n",
      "Arguments: ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menghitung BERTScore...\n",
      "\n",
      "====================================================================================================\n",
      "HASIL EVALUASI BERTSCORE (Format Per Sample)\n",
      "====================================================================================================\n",
      "\n",
      "📌 Sample S1\n",
      "🔹 F1       : 0.663\n",
      "🔹 Precision: 0.673\n",
      "🔹 Recall   : 0.654\n",
      "🔸 Prompt   : Bisa dijelaskan berapa semester dalam setahun dan adakah semester pendek?\n",
      "🔸 Reference:\n",
      "Satu tahun akademik terdiri atas 2 (dua) semester dan perguruan tinggi dapat menyelenggarakan semester antara.\n",
      "\n",
      "(Sumber: Bab II Ketentuan Umum, Pasal 2 Daftar Istilah Ayat (38), Hal. 11 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Dalam sistem akademik yang biasa, satu tahun akademik terdiri dari dua atau tiga semester. Semester adalah periode waktu yang pendek, biasanya mengikuti tahun angka, di mana mahasiswa akan mempelajari materi tertentu selama semester tersebut. Jumlah semester dalam satu tahun berbeda-beda bergantung pada institusi pendidikan yang dilamar.\n",
      "\n",
      "Contohnya, di beberapa universiti, satu tahun akademik terdiri dari tiga semester yang panjang (semester pertama, semester kedua, dan semester ketiga). Namun, di beberapa universiti lain, satu tahun akademik terdiri dari dua semester yang panjang dan satu semester pendek (semester pertama, semester kedua, dan semester pendek).\n",
      "\n",
      "Semester pendek biasanya\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S2\n",
      "🔹 F1       : 0.656\n",
      "🔹 Precision: 0.642\n",
      "🔹 Recall   : 0.670\n",
      "🔸 Prompt   : Bisa dijelaskan keputusan menteri yang dijadikan landasan pedoman Unjaya?\n",
      "🔸 Reference:\n",
      "(1) Keputusan Menteri Pendidikan dan Kebudayaan Nomor 74/P/2021 tentang Pengakuan SKS Pembelajaran Program Kampus Merdeka, (2) Keputusan Menteri Pendidikan dan Kebudayaan Nomor 3/M/2021 tentang Indikator Kinerja Utama Perguruan Tinggi Negeri dan LLDikti.\n",
      "\n",
      "(Sumber: Bab III Dasar Hukum Pedoman, Pasal 3 Dasar Hukum Ayat (8) & (9), Hal. 14 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Unjaya adalah sebuah peraturan menteri yang dikeluarkan oleh Menteri Pendidikan dan Kebudayaan Republik Indonesia Nomor 57 Tahun 2014 tentang Pedoman Pembelajaran dan Pengajaran Sains, Teknologi, Matematika, dan Bahasa Inggris di Sekolah Dasar dan Sekolah Menengah Pertama. Peraturan ini merupakan pedoman yang disusun untuk membantu guru-guru mengajar mata pelajaran Sains, Teknologi, Matematika, dan Bahasa Inggris di sekolah dasar dan SMP dengan cara yang efektif dan efisien.\n",
      "\n",
      "Di dalam Unjaya terdapat beberapa keputusan yang dijadikan landasan, antara lain:\n",
      "\n",
      "1. Memanfaatkan pendidikan inovatif dan kreatif (Innovative and Creative Education)\n",
      "2. Memanfaat\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S3\n",
      "🔹 F1       : 0.636\n",
      "🔹 Precision: 0.627\n",
      "🔹 Recall   : 0.645\n",
      "🔸 Prompt   : Siapa saja yang dapat memberikan layanan mahasiswa di Unjaya?\n",
      "🔸 Reference:\n",
      "Layanan mahasiswa dapat diberikan oleh unit khusus atau terintegrasi dalam pengelolaan perguruan tinggi.\n",
      "\n",
      "(Sumber: Bab V Administrasi dan Registrasi Akademik, Pasal 12 Layanan Akademik Ayat (2), Hal. 19 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Di universitas atau sekolah, beberapa orang yang dapat memberikan layanan kepada mahasiswa antara lain:\n",
      "\n",
      "1. Dosen/Guru: Mengajar materi-materi yang telah diajarkan, mengatur tugas dan ujian, menyediakan bantuan dalam mengatasi masalah akademik, dan memberikan saran dan panduan untuk mahasiswa.\n",
      "2. Dosen Pengajar Asisten (Teaching Assistant, TA): Membantu dosen dalam menyediakan bantuan dalam mengatasi masalah akademik, menyediakan bantuan dalam mengatasi tugas, dan membantu dalam mengajarkan materi.\n",
      "3. Pengurus/Staf Akademik: Menyediakan informasi tentang kursus, menyediakan bantuan dalam mengatasi masalah administrasi, dan membantu dalam mengatur jadwal kuliah.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S4\n",
      "🔹 F1       : 0.674\n",
      "🔹 Precision: 0.655\n",
      "🔹 Recall   : 0.693\n",
      "🔸 Prompt   : Apa definisi pedoman akademik menurut Unjaya?\n",
      "🔸 Reference:\n",
      "Pedoman akademik adalah jabaran dari kebijakan akademik Universitas Jenderal Achmad Yani Yogyakarta yang menjadi pedoman penyelenggaraan program akademik.\n",
      "\n",
      "(Sumber: Kata Pengantar, Hal. 3 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Pedoman akademik merupakan suatu petunjuk umum yang ditetapkan oleh lembaga pendidikan tinggi untuk mengatur, mengatur, dan mengatur proses pembelajaran dan penilaian di institusi tersebut. Pedoman akademik ini biasanya dikembangkan dan diatur oleh komite akademik atau badan pengelola akademik di institusi tersebut.\n",
      "\n",
      "Sesuai dengan Unjaya, pedoman akademik merupakan pedoman yang berfungsi untuk mengatur, mengatur, dan mengatur proses pembelajaran, penilaian, dan pengelolaan kualitas akademik di universitas tersebut. Pedoman akademik ini dikembangkan dan diatur oleh Komite Pedoman Akademik (KPA) Unjaya, yang terdiri dari para pakar akademik yang berwisudabeguru di institusi tersebut.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S5\n",
      "🔹 F1       : 0.631\n",
      "🔹 Precision: 0.618\n",
      "🔹 Recall   : 0.644\n",
      "🔸 Prompt   : Saat ini siapa pimpinan Prodi Sistem Informasi Unjaya?\n",
      "🔸 Reference:\n",
      "Ketua Prodi Sistem Informasi adalah Ahmad Hanafi, S.T., M.Eng.\n",
      "\n",
      "(Sumber: Struktur Organisasi Universitas Jenderal Achmad Yani Yogyakarta, Hal. 7 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Untuk informasi yang terbaru tentang pimpinan Prodi Sistem Informasi Universitas Negeri Jaya (Unjaya), anda bisa menghubungi lembaga pemerintahan atau departemen terkait, atau mengunjungi website resmi Universitas Negeri Jaya untuk informasi terkini. Karena informasi ini dapat berubah-ubah, anda harus mengambil langkah langkah tersebut untuk menghubungi lembaga terkait atau mengunjungi website resmi Universitas Negeri Jaya untuk informasi yang terbaru.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S6\n",
      "🔹 F1       : 0.658\n",
      "🔹 Precision: 0.644\n",
      "🔹 Recall   : 0.672\n",
      "🔸 Prompt   : Apa tugas dan posisi Wakil Rektor I dalam pedoman ini?\n",
      "🔸 Reference:\n",
      "Wakil Rektor I (Warek I) adalah unsur pimpinan yang membantu Rektor dalam bidang Akademik, Kemahasiswaan, Alumni, dan Pusat Karier.\n",
      "\n",
      "(Sumber: Bab II Ketentuan Umum, Pasal 2 Daftar Istilah Ayat (19), Hal. 10 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Dalam pedoman ini, Wakil Rektor I merupakan seorang pemangku tanggung jawab yang membantu Rektor dalam mengelola dan menyelenggarakan operasi dan kegiatan universitas. Wakil Rektor I memiliki beberapa tugas dan posisi yang terkait, seperti:\n",
      "\n",
      "1. Membantu Rektor dalam mengelola dan menyelenggarakan kegiatan dan operasi universitas.\n",
      "2. Membantu Rektor dalam mengurus tuntutan dan permohonan masyarakat.\n",
      "3. Membantu Rektor dalam mengurus kegiatan akademik, administrasi, dan keuangan.\n",
      "4. Membantu Rektor dalam mengurus kegiatan pengajaran dan pembelajaran.\n",
      "5. Membantu Rektor dalam mengurus kegiatan kemahasiswaan.\n",
      "6. Membantu Rektor dalam\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S7\n",
      "🔹 F1       : 0.645\n",
      "🔹 Precision: 0.626\n",
      "🔹 Recall   : 0.664\n",
      "🔸 Prompt   : Langkah apa saja yang harus dilakukan sebelum pengisian KRS di Unjaya?\n",
      "🔸 Reference:\n",
      "\"Sebelum pengisian KRS, setiap mahasiswa wajib: a) melakukan pemutakhiran data induk mahasiswa (profil/biodata), b) melakukan pemutakhiran data aktivitas untuk pengisian SKPI, dan c) melakukan bimbingan dengan Dosen Pembimbing Akademik (DPA).\"\n",
      "\n",
      "(Sumber: Bab V Administrasi dan Registrasi Akademik, Pasal 10 Registrasi Akademik Ayat (2), Hal. 18 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Sebelum melakukan pengisian KRS (Kartu Rencana Studi) di Universitas Negeri Jayapura (Unjaya), anda dapat melakukan langkah-langkah berikut:\n",
      "\n",
      "1. Pastikan Anda sudah mendapatkan akreditasi Mahasiswa Baru (MB) atau Mahasiswa Baru Bimbingan Akademik (MBBA) dari Universitas Negeri Jayapura.\n",
      "\n",
      "2. Pastikan Anda telah mendapatkan laporan Nilai (Transkrip Nilai) dari sekolah asal Anda.\n",
      "\n",
      "3. Pastikan Anda telah memilih program studi yang sesuai dengan minat dan kemampuan Anda.\n",
      "\n",
      "4. Buka akun di portal online Unjaya (http://portal.unjaya.ac.id/) atau aplikasi mobile Unjaya.\n",
      "\n",
      "5. Login ke akun Anda di portal online Unjaya.\n",
      "\n",
      "6. Klik menu \"Pengisian KRS\" di menu ut\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S8\n",
      "🔹 F1       : 0.638\n",
      "🔹 Precision: 0.619\n",
      "🔹 Recall   : 0.658\n",
      "🔸 Prompt   : Berapa batas nilai minimal tugas akhir untuk syarat kelulusan di Unjaya?\n",
      "🔸 Reference:\n",
      "Nilai kelulusan tugas akhir tidak boleh kurang dari batas minimal yang ditentukan oleh program studi.\n",
      "\n",
      "(Sumber: Bab X Tugas Akhir, Yudisium, Wisuda, Pemberian Gelar, dan Dokumen Lulusan, Pasal 47 Yudisium Ayat (4b), Hal. 38 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Untuk mendapatkan informasi yang lebih akurat, mohon mengetahui lembar persyaratan kelulusan di Universitas Jaya (Unjaya). Namun, umumnya, di beberapa universitas, syarat kelulusan mencapai kuota lulusan adalah:\n",
      "\n",
      "1. Nilai rata-rata tugas akhir (Tugas, UTS, dan UAS) minimal 70,00-75,00.\n",
      "2. Nilai rata-rata semester minimal 70,00-75,00.\n",
      "3. Nilai akhir semester minimal 65,00-70,00.\n",
      "4. Nilai akhir akhir mata kuliah minimal 60,00.\n",
      "5. Nilai akhir praktikum minimal 60,00.\n",
      "6. Nilai nilai akhir praktikum klinis minimal 60,00.\n",
      "\n",
      "Keseluruhan nilai akhir minimal ad\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S9\n",
      "🔹 F1       : 0.642\n",
      "🔹 Precision: 0.635\n",
      "🔹 Recall   : 0.649\n",
      "🔸 Prompt   : Layanan minimal untuk mahasiswa yang wajib disediakan Unjaya apa saja?\n",
      "🔸 Reference:\n",
      "Layanan mahasiswa minimal meliputi: a) administrasi akademik, b) bimbingan konseling, c) kesehatan, dan d) keperluan dasar untuk mahasiswa berkebutuhan khusus.\n",
      "\n",
      "(Sumber: Bab V Administrasi dan Registrasi Akademik, Pasal 12 Layanan Akademik Ayat (1), Hal. 19 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Berikut ini beberapa layanan minimal yang wajib disediakan oleh sekolah atau universitas Unjaya (hanya sebagai contoh dan tidak lengkap):\n",
      "\n",
      "1. Pendidikan: Sekolah atau universitas harus memberikan pendidikan yang terstruktur, terintegrasi, dan terakreditasi. Ini termasuk kuliah, praktikum, seminar, dan diskusi.\n",
      "\n",
      "2. Sarana dan Prasarana: Harus ada sarana dan prasarana yang sesuai dengan kebutuhan mahasiswa, seperti ruangan, peralatan, perlengkapan, dan fasilitas teknologi informasi.\n",
      "\n",
      "3. Dosen: Harus ada dosen yang ahli, profesional, dan terlisuhur dalam mengajar, mengajar, dan memberikan bimbingan.\n",
      "\n",
      "4. Pengelolaan Siswa: Harus ada sistem yang efisien untuk\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S10\n",
      "🔹 F1       : 0.652\n",
      "🔹 Precision: 0.656\n",
      "🔹 Recall   : 0.648\n",
      "🔸 Prompt   : Apa peran dan posisi Dekan menurut pedoman ini?\n",
      "🔸 Reference:\n",
      "Dekan adalah pimpinan tertinggi fakultas dalam lingkungan Universitas Jenderal Achmad Yani Yogyakarta (Unjaya) yang berwenang dan bertanggung jawab terhadap penyelenggaraan fakultas, diangkat oleh Rektor dan bertanggung jawab secara langsung kepada Rektor.\n",
      "\n",
      "(Sumber: Bab II Ketentuan Umum, Pasal 2 Daftar Istilah Ayat (22), Hal. 11 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Dalam pedoman ini, Dekan merupakan posisi yang terpenting di dalam suatu lembaga pendidikan tingkat tinggi, sekolah tinggi, atau universitas. Dekan memiliki beberapa fungsi dan tanggung jawab yang berikut:\n",
      "\n",
      "1. Mengurus dan menyusun strategi, tujuan, dan tindakan institusi pendidikan tersebut.\n",
      "2. Menyediakan pengaruh dan pengaruh dalam membentuk dan mengimplementasikan kebijakan institusi pendidikan.\n",
      "3. Menyediakan pengaruh dalam pengambilan keputusan yang berpengaruh terhadap kelancaran dan keberhasilan institusi pendidikan.\n",
      "4. Menyediakan pengaruh dalam pengelolaan sumber daya manusia, termasuk tenaga ahli, dosen, dan staff lainnya.\n",
      "5. Men\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S11\n",
      "🔹 F1       : 0.643\n",
      "🔹 Precision: 0.632\n",
      "🔹 Recall   : 0.655\n",
      "🔸 Prompt   : Di mana aturan teknis pelaksanaan wisuda di Unjaya ditetapkan?\n",
      "🔸 Reference:\n",
      "Ketentuan lebih lanjut tentang pelaksanaan wisuda ditetapkan melalui Keputusan Rektor.\n",
      "\n",
      "(Sumber: Bab X Tugas Akhir, Yudisium, Wisuda, Pemberian Gelar, dan Dokumen Lulusan, Pasal 48 Wisuda Ayat (4), Hal. 39 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Aturan teknis pelaksanaan Wisuda di Universitas Negeri Jayapura (Unjaya) ditetapkan dalam Peraturan Rektor (Perretno) dan Peraturan Fakultas masing-masing. Beberapa contoh adalah:\n",
      "\n",
      "1. Perretno Unjaya Nomor 2 Tahun 2020 tentang Pelaksanaan Akademik.\n",
      "2. Perretno Fakultas Hukum Unjaya Nomor 1 Tahun 2021 tentang Pelaksanaan Wisuda.\n",
      "3. Perretno Fakultas Ilmu Sosial dan Ilmu Politik Unjaya Nomor 1 Tahun 2021 tentang Pelaksanaan Wisuda.\n",
      "4. Perretno Fakultas Ilmu Komputer dan Teknologi Informasi Unjaya Nomor 1 Tahun 2021 tentang Pelaksanaan Wisuda.\n",
      "\n",
      "Selain itu, juga ada peraturan-peraturan yang berlisensi dari Kement\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S12\n",
      "🔹 F1       : 0.623\n",
      "🔹 Precision: 0.603\n",
      "🔹 Recall   : 0.645\n",
      "🔸 Prompt   : Di mana bisa ditemukan aturan tambahan tentang program AYM Unjaya?\n",
      "🔸 Reference:\n",
      "Ketentuan lebih lanjut mengenai pengembangan karakter Achmad Yani Muda diatur dalam Surat Keputusan Rektor.\n",
      "\n",
      "(Sumber: Bab IV Sistem Penerimaan dan Pendaftaran Mahasiswa, Pasal 8 Pengembangan Karakter Achmad Yani Muda Ayat (3), Hal. 17 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Aturan tambahan tentang program AYM Unjaya dapat ditemukan di sumber-sumber berikut:\n",
      "\n",
      "1. Website resmi Program AYM Unjaya: <https://aym.unj.ac.id/>\n",
      "2. Dokumen Peraturan Menteri Pendidikan dan Kebudayaan Nomor 206/M/2020 tentang Pelaksanaan Program AYM Unjaya.\n",
      "3. Dokumen Peraturan Rektor Unjaya Nomor 522/UN6.K/2020 tentang Pelaksanaan Program AYM Unjaya.\n",
      "4. Dokumen Peraturan Rektor Unjaya Nomor 523/UN6.K/2020 tentang Pelaksanaan Program AYM Unjaya.\n",
      "5. Dokumen Peraturan Rektor Unjaya Nomor 524/UN6.K/2020 tentang Pelaksanaan Program AYM Unjaya.\n",
      "6. Dokumen Peraturan Rektor Un\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S13\n",
      "🔹 F1       : 0.599\n",
      "🔹 Precision: 0.594\n",
      "🔹 Recall   : 0.604\n",
      "🔸 Prompt   : Apa yang terjadi jika mahasiswa tidak cukup hadir sesuai persentase minimal?\n",
      "🔸 Reference:\n",
      "Kehilangan hak ikut ujian akhir semester dan dinyatakan mendapat nilai E, kecuali jika ada surat ijin atau surat keterangan sakit.\n",
      "\n",
      "(Sumber: Bab VIII Sistem Pembelajaran, Pasal 31 Pelaksanaan Proses Pembelajaran Ayat (18)–(19), Hal. 29 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Jika seorang mahasiswa tidak cukup hadir sesuai dengan persentase minimal yang ditetapkan oleh lembaga pendidikan, maka dapat terjadi beberapa konsekuensi sebagai berikut:\n",
      "\n",
      "1. Absensi Tidak Terhad: Absensi yang tidak terhad ke arah yang teliti dapat menyebabkan pembuangan dari program studi.\n",
      "\n",
      "2. Kenaikan Biaya: Jika persentase hadir yang teliti tidak terpenuhi, maka bisa terjadi penghitungan biaya yang lebih tinggi.\n",
      "\n",
      "3. Nilai Akhir Reduksi: Ketidakhadiran dapat menyebabkan reduksi nilai akhir.\n",
      "\n",
      "4. Pengurangan Prestasi: Prestasi akademik yang rendah dapat menyebabkan pengurangan prestasi akademik.\n",
      "\n",
      "5. Penghapusan Daftar Mahasiswa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S14\n",
      "🔹 F1       : 0.650\n",
      "🔹 Precision: 0.632\n",
      "🔹 Recall   : 0.669\n",
      "🔸 Prompt   : Bagaimana definisi Kalender Akademik menurut Unjaya?\n",
      "🔸 Reference:\n",
      "Kalender Akademik adalah panduan waktu kegiatan akademik yang disepakati dan ditaati oleh semua pihak selama satu tahun akademik, diterbitkan setiap akhir bulan Juni tahun berjalan.\n",
      "\n",
      "(Sumber: Bab II Ketentuan Umum, Pasal 2 Daftar Istilah Ayat (51), Hal. 12 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Kalender Akademik dalam Unjaya (Universitas Jenderal Achmad Yani) adalah rencana tahunan mengenai penyusunan, pelaksanaan, dan penyelesaian tugas-tugas akademik di universitas tersebut. Dalam kalender akademik tersebut terdapat beberapa tahap yang terdiri dari:\n",
      "\n",
      "1. Periode Pendaftaran Mahasiswa Baru: Tahap ini meliputi periode pendaftaran baru bagi mahasiswa yang baru mau masuk ke universitas.\n",
      "\n",
      "2. Periode Pengajuan Surat Keterangan Lulus (SKL): Mahasiswa yang sedang melakukan studi di Unjaya harus memiliki SKL sebelum diterima.\n",
      "\n",
      "3. Periode Pengajuan Pendaftaran Mata Kuliah: Mahasiswa harus melakukan pengajuan mata kuliah setiap semester.\n",
      "\n",
      "4. Periode Peng\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "📌 Sample S15\n",
      "🔹 F1       : 0.623\n",
      "🔹 Precision: 0.627\n",
      "🔹 Recall   : 0.619\n",
      "🔸 Prompt   : Siapa Dekan Fakultas Ekonomi dan Sosial Unjaya?\n",
      "🔸 Reference:\n",
      "Dekan FES adalah Edhy Tri Cahyono, S.Si., M.M.\n",
      "\n",
      "(Sumber: Struktur Organisasi Universitas Jenderal Achmad Yani Yogyakarta, Hal. 8 (Pedoman Akademik Unjaya 2024))\n",
      "🔸 Prediksi :\n",
      "Untuk informasi terbaru tentang dekan Fakultas Ekonomi dan Sosial Universitas Negeri Jayapura (Unjaya), mohon perhatikan pengumuman yang diterbitkan secara terbatas oleh Universitas Negeri Jayapura atau melalui website resmi fakultas tersebut. Kami tidak dapat memberikan informasi terbaru secara langsung, namun kami akan membantu Anda menemukan informasi tersebut dengan cara memberikan tautan ke situs web resmi Universitas Negeri Jayapura atau Fakultas Ekonomi dan Sosial Unjaya.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "RATA-RATA SKOR:\n",
      "   F1 Score  : 0.6421\n",
      "   Precision : 0.6323\n",
      "   Recall    : 0.6525\n",
      "\n",
      "SAMPLE TERBAIK (F1: 0.674):\n",
      "   Prompt: Apa definisi pedoman akademik menurut Unjaya?\n",
      "   Reference: Pedoman akademik adalah jabaran dari kebijakan akademik Universitas Jenderal Achmad Yani Yogyakarta yang menjadi pedoman penyelenggaraan program akademik.\n",
      "\n",
      "(Sumber: Kata Pengantar, Hal. 3 (Pedoman Akademik Unjaya 2024))\n",
      "   Prediksi: Pedoman akademik merupakan suatu petunjuk umum yang ditetapkan oleh lembaga pendidikan tinggi untuk mengatur, mengatur, dan mengatur proses pembelajaran dan penilaian di institusi tersebut. Pedoman akademik ini biasanya dikembangkan dan diatur oleh komite akademik atau badan pengelola akademik di institusi tersebut.\n",
      "\n",
      "Sesuai dengan Unjaya, pedoman akademik merupakan pedoman yang berfungsi untuk mengatur, mengatur, dan mengatur proses pembelajaran, penilaian, dan pengelolaan kualitas akademik di universitas tersebut. Pedoman akademik ini dikembangkan dan diatur oleh Komite Pedoman Akademik (KPA) Unjaya, yang terdiri dari para pakar akademik yang berwisudabeguru di institusi tersebut.\n",
      "\n",
      "SAMPLE TERBURUK (F1: 0.599):\n",
      "   Prompt: Apa yang terjadi jika mahasiswa tidak cukup hadir sesuai persentase minimal?\n",
      "   Reference: Kehilangan hak ikut ujian akhir semester dan dinyatakan mendapat nilai E, kecuali jika ada surat ijin atau surat keterangan sakit.\n",
      "\n",
      "(Sumber: Bab VIII Sistem Pembelajaran, Pasal 31 Pelaksanaan Proses Pembelajaran Ayat (18)–(19), Hal. 29 (Pedoman Akademik Unjaya 2024))\n",
      "   Prediksi: Jika seorang mahasiswa tidak cukup hadir sesuai dengan persentase minimal yang ditetapkan oleh lembaga pendidikan, maka dapat terjadi beberapa konsekuensi sebagai berikut:\n",
      "\n",
      "1. Absensi Tidak Terhad: Absensi yang tidak terhad ke arah yang teliti dapat menyebabkan pembuangan dari program studi.\n",
      "\n",
      "2. Kenaikan Biaya: Jika persentase hadir yang teliti tidak terpenuhi, maka bisa terjadi penghitungan biaya yang lebih tinggi.\n",
      "\n",
      "3. Nilai Akhir Reduksi: Ketidakhadiran dapat menyebabkan reduksi nilai akhir.\n",
      "\n",
      "4. Pengurangan Prestasi: Prestasi akademik yang rendah dapat menyebabkan pengurangan prestasi akademik.\n",
      "\n",
      "5. Penghapusan Daftar Mahasiswa\n",
      "\n",
      "Hasil sudah di-log ke wandb\n",
      "\n",
      "Evaluasi selesai!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
      "    ColabKernelApp.launch_instance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2981, in run_cell\n",
      "    self.events.trigger('post_run_cell', result)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/events.py\", line 89, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 592, in _post_run_cell_hook\n",
      "    self._logger.info(\"resuming backend\")\n",
      "Message: 'resuming backend'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Hitung BERTScore\n",
    "print(\"\\nMenghitung BERTScore...\")\n",
    "\n",
    "try:\n",
    "    results = bertscore.compute(\n",
    "        predictions=generated,\n",
    "        references=references,\n",
    "        lang=\"id\"  # Bahasa Indonesia\n",
    "    )\n",
    "    \n",
    "    # Hitung rata-rata\n",
    "    avg_f1 = np.mean(results[\"f1\"])\n",
    "    avg_precision = np.mean(results[\"precision\"])\n",
    "    avg_recall = np.mean(results[\"recall\"])\n",
    "    \n",
    "    # Buat DataFrame lengkap tanpa pemotongan teks\n",
    "    df = pd.DataFrame({\n",
    "        'Sample': [f\"S{i+1}\" for i in range(len(generated))],\n",
    "        'F1': [f\"{score:.3f}\" for score in results['f1']],\n",
    "        'Precision': [f\"{score:.3f}\" for score in results['precision']],\n",
    "        'Recall': [f\"{score:.3f}\" for score in results['recall']],\n",
    "        'Prompt': prompts,\n",
    "        'Reference': references,\n",
    "        'Prediksi': generated\n",
    "    })\n",
    "    \n",
    "    # Tampilkan semua kolom dan lebar maksimal untuk teks\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"HASIL EVALUASI BERTSCORE (Format Per Sample)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"\\n📌 Sample {row['Sample']}\")\n",
    "        print(f\"🔹 F1       : {row['F1']}\")\n",
    "        print(f\"🔹 Precision: {row['Precision']}\")\n",
    "        print(f\"🔹 Recall   : {row['Recall']}\")\n",
    "        print(f\"🔸 Prompt   : {row['Prompt']}\")\n",
    "        print(f\"🔸 Reference:\\n{row['Reference']}\")\n",
    "        print(f\"🔸 Prediksi :\\n{row['Prediksi']}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    \n",
    "    # Tampilkan rata-rata\n",
    "    print(f\"\\nRATA-RATA SKOR:\")\n",
    "    print(f\"   F1 Score  : {avg_f1:.4f}\")\n",
    "    print(f\"   Precision : {avg_precision:.4f}\")\n",
    "    print(f\"   Recall    : {avg_recall:.4f}\")\n",
    "    \n",
    "    # Tampilkan sample terbaik dan terburuk\n",
    "    best_idx = np.argmax(results['f1'])\n",
    "    worst_idx = np.argmin(results['f1'])\n",
    "    \n",
    "    print(f\"\\nSAMPLE TERBAIK (F1: {results['f1'][best_idx]:.3f}):\")\n",
    "    print(f\"   Prompt: {prompts[best_idx]}\")\n",
    "    print(f\"   Reference: {references[best_idx]}\")\n",
    "    print(f\"   Prediksi: {generated[best_idx]}\")\n",
    "    \n",
    "    print(f\"\\nSAMPLE TERBURUK (F1: {results['f1'][worst_idx]:.3f}):\")\n",
    "    print(f\"   Prompt: {prompts[worst_idx]}\")\n",
    "    print(f\"   Reference: {references[worst_idx]}\")\n",
    "    print(f\"   Prediksi: {generated[worst_idx]}\")\n",
    "    \n",
    "    # Log ke wandb\n",
    "    if wandb.run:\n",
    "        wandb.log({\n",
    "            \"eval/bertscore_f1\": avg_f1,\n",
    "            \"eval/bertscore_precision\": avg_precision,\n",
    "            \"eval/bertscore_recall\": avg_recall\n",
    "        })\n",
    "        print(\"\\nHasil sudah di-log ke wandb\")\n",
    "    \n",
    "    print(\"\\nEvaluasi selesai!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saat menghitung BERTScore: {e}\")\n",
    "    print(\"Coba gunakan model default atau cek koneksi internet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T04:27:21.861352Z",
     "iopub.status.busy": "2025-07-17T04:27:21.860987Z",
     "iopub.status.idle": "2025-07-17T04:27:21.900967Z",
     "shell.execute_reply": "2025-07-17T04:27:21.899865Z",
     "shell.execute_reply.started": "2025-07-17T04:27:21.861327Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "BERTScore Classification Report\n",
      "==================================================\n",
      "Range F1 Score  Jumlah Sampel Persentase (%)\n",
      "        < 0.60              1           6.7%\n",
      "     0.60–0.69             14          93.3%\n",
      "\n",
      "Statistik Ringkasan F1:\n",
      "  Mean   : 0.6421\n",
      "  Median : 0.6432\n",
      "  Min    : 0.5989\n",
      "  Max    : 0.6737\n",
      "  Std Dev: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
      "    ColabKernelApp.launch_instance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3148, in run_cell_async\n",
      "    self.events.trigger('pre_run_cell', info)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/events.py\", line 89, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 575, in _pre_run_cell_hook\n",
      "    if self.notebook and self.notebook.save_ipynb():\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/jupyter.py\", line 404, in save_ipynb\n",
      "    logger.info(\"not saving jupyter notebook\")\n",
      "Message: 'not saving jupyter notebook'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
      "    ColabKernelApp.launch_instance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3148, in run_cell_async\n",
      "    self.events.trigger('pre_run_cell', info)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/events.py\", line 89, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 581, in _pre_run_cell_hook\n",
      "    self._logger.info(\"pausing backend\")\n",
      "Message: 'pausing backend'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
      "    ColabKernelApp.launch_instance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2981, in run_cell\n",
      "    self.events.trigger('post_run_cell', result)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/events.py\", line 89, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 592, in _post_run_cell_hook\n",
      "    self._logger.info(\"resuming backend\")\n",
      "Message: 'resuming backend'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "# Konversi F1 ke array NumPy\n",
    "f1_scores = np.array(results[\"f1\"])\n",
    "\n",
    "# Klasifikasi skor F1\n",
    "def classify_f1(f1):\n",
    "    if f1 >= 0.70:\n",
    "        return \"≥ 0.70\"\n",
    "    elif f1 >= 0.60:\n",
    "        return \"0.60–0.69\"\n",
    "    else:\n",
    "        return \"< 0.60\"\n",
    "\n",
    "f1_categories = [classify_f1(score) for score in f1_scores]\n",
    "\n",
    "# Hitung distribusi kategori\n",
    "from collections import Counter\n",
    "\n",
    "f1_distribution = Counter(f1_categories)\n",
    "total_samples = len(f1_scores)\n",
    "\n",
    "# Buat DataFrame laporan klasifikasi\n",
    "classification_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Range F1 Score\": k,\n",
    "        \"Jumlah Sampel\": v,\n",
    "        \"Persentase (%)\": f\"{(v/total_samples)*100:.1f}%\"\n",
    "    }\n",
    "    for k, v in sorted(f1_distribution.items(), reverse=True)\n",
    "])\n",
    "\n",
    "# Statistik ringkasan\n",
    "f1_mean = np.mean(f1_scores)\n",
    "f1_median = np.median(f1_scores)\n",
    "f1_min = np.min(f1_scores)\n",
    "f1_max = np.max(f1_scores)\n",
    "f1_std = np.std(f1_scores)\n",
    "\n",
    "# Tampilkan\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BERTScore Classification Report\")\n",
    "print(\"=\"*50)\n",
    "print(classification_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nStatistik Ringkasan F1:\")\n",
    "print(f\"  Mean   : {f1_mean:.4f}\")\n",
    "print(f\"  Median : {f1_median:.4f}\")\n",
    "print(f\"  Min    : {f1_min:.4f}\")\n",
    "print(f\"  Max    : {f1_max:.4f}\")\n",
    "print(f\"  Std Dev: {f1_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T03:24:26.866587Z",
     "iopub.status.busy": "2025-07-17T03:24:26.865784Z",
     "iopub.status.idle": "2025-07-17T03:24:30.946349Z",
     "shell.execute_reply": "2025-07-17T03:24:30.945570Z",
     "shell.execute_reply.started": "2025-07-17T03:24:26.866559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /kaggle/working/adapter-pedoman-akademik\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "chat template saved in /kaggle/working/adapter-pedoman-akademik/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/adapter-pedoman-akademik/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/adapter-pedoman-akademik/special_tokens_map.json\n",
      "Saving model checkpoint to /kaggle/working/results-pedoman-akademik\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/special_tokens_map.json\n",
      "chat template saved in /kaggle/working/adapter-pedoman-akademik/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/adapter-pedoman-akademik/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/adapter-pedoman-akademik/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/adapter-pedoman-akademik/tokenizer_config.json',\n",
       " '/kaggle/working/adapter-pedoman-akademik/special_tokens_map.json',\n",
       " '/kaggle/working/adapter-pedoman-akademik/chat_template.jinja',\n",
       " '/kaggle/working/adapter-pedoman-akademik/tokenizer.model',\n",
       " '/kaggle/working/adapter-pedoman-akademik/added_tokens.json',\n",
       " '/kaggle/working/adapter-pedoman-akademik/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simpan adapter model (LoRA) ke folder adapter_dir\n",
    "adapter_dir = \"/kaggle/working/adapter-pedoman-akademik\"\n",
    "trainer.save_model(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T03:26:10.411293Z",
     "iopub.status.busy": "2025-07-17T03:26:10.410925Z",
     "iopub.status.idle": "2025-07-17T03:26:19.908838Z",
     "shell.execute_reply": "2025-07-17T03:26:19.907983Z",
     "shell.execute_reply.started": "2025-07-17T03:26:10.411268Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/config.json\n",
      "Configuration saved in /kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/model.safetensors.index.json.\n",
      "chat template saved in /kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/tokenizer_config.json',\n",
       " '/kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/special_tokens_map.json',\n",
       " '/kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/chat_template.jinja',\n",
       " '/kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/tokenizer.model',\n",
       " '/kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/added_tokens.json',\n",
       " '/kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge adapter ke base model\n",
    "merged_model = trainer.model.merge_and_unload()\n",
    "\n",
    "# Nama dan path full model\n",
    "full_model_name = f\"{new_model_name}-merged\"\n",
    "full_model_dir = f\"/kaggle/working/{full_model_name}\"\n",
    "\n",
    "# Simpan full model dan tokenizer ke lokal\n",
    "merged_model.save_pretrained(full_model_dir)\n",
    "tokenizer.save_pretrained(full_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T03:26:33.024382Z",
     "iopub.status.busy": "2025-07-17T03:26:33.023699Z",
     "iopub.status.idle": "2025-07-17T03:28:26.900728Z",
     "shell.execute_reply": "2025-07-17T03:28:26.899934Z",
     "shell.execute_reply.started": "2025-07-17T03:26:33.024356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/tmpt2tjx1at/config.json\n",
      "Configuration saved in /tmp/tmpt2tjx1at/generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/tmpt2tjx1at/model.safetensors.index.json.\n",
      "Uploading the following files to riakrst/mistral-7b-pedoman-akademik-unjaya-merged: generation_config.json,config.json,README.md,model-00001-of-00002.safetensors,model-00002-of-00002.safetensors,model.safetensors.index.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a37abb01644793b5a3209d42a3d3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8362e7f6b0474105b8156c596a6d22e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/537M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e714ab32da48a7b035c36f9cc4cfdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5427eac82649fc8013d262dbed0833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chat template saved in /tmp/tmp6tcc5ybv/chat_template.jinja\n",
      "tokenizer config file saved in /tmp/tmp6tcc5ybv/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmp6tcc5ybv/special_tokens_map.json\n",
      "Uploading the following files to riakrst/mistral-7b-pedoman-akademik-unjaya-merged: special_tokens_map.json,tokenizer.json,tokenizer_config.json,chat_template.jinja,tokenizer.model,README.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767d3ac8c1634139b7336e948df6de37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model berhasil disimpan di: /kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged\n",
      "Full model berhasil diupload ke: https://huggingface.co/riakrst/mistral-7b-pedoman-akademik-unjaya-merged\n"
     ]
    }
   ],
   "source": [
    "# sizenya gede banget\n",
    "# # Upload model hasil merge ke Hugging Face Hub\n",
    "# merged_model.push_to_hub(full_model_name)\n",
    "# tokenizer.push_to_hub(full_model_name)\n",
    "# print(f\"Full model berhasil disimpan di: {full_model_dir}\")\n",
    "# print(f\"Full model berhasil diupload ke: https://huggingface.co/{full_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T03:31:51.066167Z",
     "iopub.status.busy": "2025-07-17T03:31:51.065771Z",
     "iopub.status.idle": "2025-07-17T03:31:52.678307Z",
     "shell.execute_reply": "2025-07-17T03:31:52.677607Z",
     "shell.execute_reply.started": "2025-07-17T03:31:51.066142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /kaggle/working/results-pedoman-akademik\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "chat template saved in /kaggle/working/results-pedoman-akademik/chat_template.jinja\n",
      "tokenizer config file saved in /kaggle/working/results-pedoman-akademik/tokenizer_config.json\n",
      "Special tokens file saved in /kaggle/working/results-pedoman-akademik/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e9dd81f4774fb09f4216a1d3b8902b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter (LoRA) juga berhasil diupload ke: https://huggingface.co/riakrst/mistral-7b-pedoman-akademik-unjaya\n"
     ]
    }
   ],
   "source": [
    "# Upload adapter jika disetel di konfigurasi\n",
    "if sft_config.push_to_hub:\n",
    "    trainer.push_to_hub()\n",
    "    print(f\"Adapter (LoRA) juga berhasil diupload ke: https://huggingface.co/{new_model_name}\")\n",
    "else:\n",
    "    print(\"Adapter tidak diupload otomatis (karena sft_config.push_to_hub = False)\")\n",
    "    print(f\"Untuk upload manual: trainer.model.push_to_hub('{new_model_name}')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T03:32:19.314613Z",
     "iopub.status.busy": "2025-07-17T03:32:19.314070Z",
     "iopub.status.idle": "2025-07-17T03:32:19.324217Z",
     "shell.execute_reply": "2025-07-17T03:32:19.323391Z",
     "shell.execute_reply.started": "2025-07-17T03:32:19.314589Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROSES MERGE DAN UPLOAD SELESAI!\n",
      "============================================================\n",
      "Full model tersedia di:   https://huggingface.co/riakrst/mistral-7b-pedoman-akademik-unjaya-merged\n",
      "LoRA adapter :  https://huggingface.co/riakrst/mistral-7b-pedoman-akademik-unjaya\n",
      "\n",
      "Untuk inference:\n",
      "model = AutoModelForCausalLM.from_pretrained('riakrst/mistral-7b-pedoman-akademik-unjaya-merged')\n"
     ]
    }
   ],
   "source": [
    "print(\"PROSES MERGE DAN UPLOAD SELESAI!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Full model tersedia di:   https://huggingface.co/{full_model_name}\")\n",
    "print(f\"LoRA adapter :  https://huggingface.co/{new_model_name}\")\n",
    "print(\"\\nUntuk inference:\")\n",
    "print(f\"model = AutoModelForCausalLM.from_pretrained('{full_model_name}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T05:04:48.374303Z",
     "iopub.status.busy": "2025-07-17T05:04:48.373423Z",
     "iopub.status.idle": "2025-07-17T05:05:09.125058Z",
     "shell.execute_reply": "2025-07-17T05:05:09.124241Z",
     "shell.execute_reply.started": "2025-07-17T05:04:48.374265Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
      "    ColabKernelApp.launch_instance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3148, in run_cell_async\n",
      "    self.events.trigger('pre_run_cell', info)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/events.py\", line 89, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 575, in _pre_run_cell_hook\n",
      "    if self.notebook and self.notebook.save_ipynb():\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/jupyter.py\", line 404, in save_ipynb\n",
      "    logger.info(\"not saving jupyter notebook\")\n",
      "Message: 'not saving jupyter notebook'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
      "    ColabKernelApp.launch_instance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3148, in run_cell_async\n",
      "    self.events.trigger('pre_run_cell', info)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/events.py\", line 89, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 581, in _pre_run_cell_hook\n",
      "    self._logger.info(\"pausing backend\")\n",
      "Message: 'pausing backend'\n",
      "Arguments: ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33G\t/root/.cache\n",
      "33G\t/root/.cache\n",
      "357M\t/kaggle/working/adapter-pedoman-akademik\n",
      "0\t/kaggle/working/adapter-pedoman-akademik.zip\n",
      "56K\t/kaggle/working/eval_chatml.jsonl\n",
      "0\t/kaggle/working/mistral-7b-pedoman-akademik-unjaya-merged.zip\n",
      "1.1G\t/kaggle/working/results-pedoman-akademik\n",
      "4.7G\t/kaggle/working/riakrst\n",
      "692K\t/kaggle/working/train_chatml.jsonl\n",
      "480K\t/kaggle/working/wandb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1114, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1094, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
      "    ColabKernelApp.launch_instance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2981, in run_cell\n",
      "    self.events.trigger('post_run_cell', result)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/events.py\", line 89, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 592, in _post_run_cell_hook\n",
      "    self._logger.info(\"resuming backend\")\n",
      "Message: 'resuming backend'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "!du -sh /root/.cache\n",
    "!du -sh ~/.cache\n",
    "!du -sh /kaggle/working/*  # Ulangi pengecekan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T05:23:03.894291Z",
     "iopub.status.busy": "2025-07-17T05:23:03.893587Z",
     "iopub.status.idle": "2025-07-17T05:23:04.960540Z",
     "shell.execute_reply": "2025-07-17T05:23:04.959853Z",
     "shell.execute_reply.started": "2025-07-17T05:23:03.894263Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16G\t/root/.cache/uv\n",
      "15G\t/root/.cache/huggingface\n",
      "1.7G\t/root/.cache/pip\n",
      "310M\t/root/.cache/jedi\n",
      "56M\t/root/.cache/node-gyp\n",
      "96K\t/root/.cache/matplotlib\n",
      "20K\t/root/.cache/wandb\n"
     ]
    }
   ],
   "source": [
    "!du -sh /root/.cache/* | sort -hr | head -20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T05:25:17.858664Z",
     "iopub.status.busy": "2025-07-17T05:25:17.857777Z",
     "iopub.status.idle": "2025-07-17T05:25:38.474405Z",
     "shell.execute_reply": "2025-07-17T05:25:38.473404Z",
     "shell.execute_reply.started": "2025-07-17T05:25:17.858627Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Adapter berhasil diarsipkan: /kaggle/working/adapter-pedoman-akademik.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Lokasi folder adapter\n",
    "adapter_dir = \"/kaggle/working/adapter-pedoman-akademik\"\n",
    "adapter_zip = \"/kaggle/working/adapter-pedoman-akademik.zip\"\n",
    "\n",
    "# Buat zip dari folder adapter\n",
    "shutil.make_archive(adapter_zip.replace(\".zip\", \"\"), 'zip', adapter_dir)\n",
    "print(f\"✅ Adapter berhasil diarsipkan: {adapter_zip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T05:26:07.575342Z",
     "iopub.status.busy": "2025-07-17T05:26:07.575028Z",
     "iopub.status.idle": "2025-07-17T05:33:17.178016Z",
     "shell.execute_reply": "2025-07-17T05:33:17.176986Z",
     "shell.execute_reply.started": "2025-07-17T05:26:07.575318Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Full merged model berhasil diarsipkan: /kaggle/working/mistral-7b-pedoman-akademik-unjaya-merged.zip\n"
     ]
    }
   ],
   "source": [
    "# Model terlalu besar untuk diunduh lokal\n",
    "# Lokasi folder full model \n",
    "full_model_dir = \"/kaggle/working/riakrst/mistral-7b-pedoman-akademik-unjaya-merged\"\n",
    "full_model_zip = \"/kaggle/working/mistral-7b-pedoman-akademik-unjaya-merged.zip\"\n",
    "\n",
    "# Buat zip dari folder full model\n",
    "shutil.make_archive(full_model_zip.replace(\".zip\", \"\"), 'zip', full_model_dir)\n",
    "print(f\"✅ Full merged model berhasil diarsipkan: {full_model_zip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tmzcNca43zj"
   },
   "source": [
    "# 7. SIMPLE INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T05:08:12.321160Z",
     "iopub.status.busy": "2025-07-17T05:08:12.320774Z",
     "iopub.status.idle": "2025-07-17T05:08:45.982474Z",
     "shell.execute_reply": "2025-07-17T05:08:45.981761Z",
     "shell.execute_reply.started": "2025-07-17T05:08:12.321126Z"
    },
    "id": "VfZeGrZ_5EEl",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contoh Inference:\n",
      "==================================================\n",
      "\n",
      "1. Pertanyaan: Bagaimana cara mengajukan cuti akademik?\n",
      "   Jawaban: Untuk mengajukan cuti akademik di sekolah atau perusahaan, anda perlu menyusun surat permohonan cuti akademik yang sesuai. Berikut adalah langkah-langkah umum yang dapat dilakukan:\n",
      "\n",
      "1. Tempatkan surat permohonan cuti akademik di bawah anda.\n",
      "2. Tulis alamat, nomor telepon, dan email anda di bagian atas surat.\n",
      "3. Tulis nama institusi atau perusahaan yang anda kerjakan di bagian atas surat.\n",
      "4. Tulis nama pengurus atau pengelola yang bertanggung jawab atas anda di bagian atas surat.\n",
      "5. Tulis tanggal surat di bagian atas surat.\n",
      "6. Tulis tujuan cuti akademik anda di bagian pertama surat. Contoh: \"Saya mengajukan cuti ak\n",
      "--------------------------------------------------\n",
      "\n",
      "2. Pertanyaan: Berapa lama masa studi maksimal untuk S1?\n",
      "   Jawaban: Masa studi maksimal untuk S1 di Indonesia adalah 4 tahun atau 8 semester, mengikut Aturan Ketetapan Pemerintah Nomor 2 Tahun 2012 tentang Peraturan Pemerintah Nomor 123 Tahun 2012 tentang Standar Kualifikasi Pendidikan Diploma Strata I dan II. Namun, masa studi mungkin lebih panjang bila mengambil beban mata pelajaran yang lebih ringkas atau bila memerlukan lamaran khas.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Contoh inference\n",
    "test_questions = [\n",
    "    \"Bagaimana cara mengajukan cuti akademik?\",\n",
    "    \"Berapa lama masa studi maksimal untuk S1?\",\n",
    "]\n",
    "\n",
    "print(\"\\nContoh Inference:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": question}]\n",
    "        \n",
    "        result = pipe(\n",
    "            messages,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        response = result[0]['generated_text'].strip()\n",
    "        \n",
    "        print(f\"\\n{i}. Pertanyaan: {question}\")\n",
    "        print(f\"   Jawaban: {response}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error pada pertanyaan {i}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T06:37:38.701095Z",
     "iopub.status.busy": "2025-07-17T06:37:38.700497Z",
     "iopub.status.idle": "2025-07-17T06:37:39.038401Z",
     "shell.execute_reply": "2025-07-17T06:37:39.037495Z",
     "shell.execute_reply.started": "2025-07-17T06:37:38.701066Z"
    },
    "id": "lDLoVzvc4A3e",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bertscore_f1</td><td>▁▁▁</td></tr><tr><td>eval/bertscore_precision</td><td>▁▁▁</td></tr><tr><td>eval/bertscore_recall</td><td>▁▁▁</td></tr><tr><td>eval/loss</td><td>█▃▂▁▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁▅▇██</td></tr><tr><td>eval/num_tokens</td><td>▁▃▅▇█</td></tr><tr><td>eval/runtime</td><td>▃▁▃▄█</td></tr><tr><td>eval/samples_per_second</td><td>▆█▆▅▁</td></tr><tr><td>eval/steps_per_second</td><td>▆█▆▆▁</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▃▂▁</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▂▁▁</td></tr><tr><td>train/mean_token_accuracy</td><td>▁▆▇██</td></tr><tr><td>train/num_tokens</td><td>▁▃▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bertscore_f1</td><td>0.64212</td></tr><tr><td>eval/bertscore_precision</td><td>0.63229</td></tr><tr><td>eval/bertscore_recall</td><td>0.65254</td></tr><tr><td>eval/loss</td><td>0.91808</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.80338</td></tr><tr><td>eval/num_tokens</td><td>239036</td></tr><tr><td>eval/runtime</td><td>57.3104</td></tr><tr><td>eval/samples_per_second</td><td>2.129</td></tr><tr><td>eval/steps_per_second</td><td>1.064</td></tr><tr><td>total_flos</td><td>1.2234108777086976e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>47</td></tr><tr><td>train/grad_norm</td><td>0.5747</td></tr><tr><td>train/learning_rate</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.9971</td></tr><tr><td>train/mean_token_accuracy</td><td>0.7967</td></tr><tr><td>train/num_tokens</td><td>239036</td></tr><tr><td>train_loss</td><td>1.49988</td></tr><tr><td>train_runtime</td><td>2268.45</td></tr><tr><td>train_samples_per_second</td><td>0.663</td></tr><tr><td>train_steps_per_second</td><td>0.021</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-leaf-6</strong> at: <a href='https://wandb.ai/riakrst-universitas-jenderal-achmad-yani-yogyakarta/Fine-tuning-Mistral-7B-Pedoman-Akademik/runs/8s24e2f5' target=\"_blank\">https://wandb.ai/riakrst-universitas-jenderal-achmad-yani-yogyakarta/Fine-tuning-Mistral-7B-Pedoman-Akademik/runs/8s24e2f5</a><br> View project at: <a href='https://wandb.ai/riakrst-universitas-jenderal-achmad-yani-yogyakarta/Fine-tuning-Mistral-7B-Pedoman-Akademik' target=\"_blank\">https://wandb.ai/riakrst-universitas-jenderal-achmad-yani-yogyakarta/Fine-tuning-Mistral-7B-Pedoman-Akademik</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250717_024340-8s24e2f5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleanup\n",
    "if wandb.run:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T06:37:29.936551Z",
     "iopub.status.busy": "2025-07-17T06:37:29.935995Z",
     "iopub.status.idle": "2025-07-17T06:37:29.952106Z",
     "shell.execute_reply": "2025-07-17T06:37:29.951545Z",
     "shell.execute_reply.started": "2025-07-17T06:37:29.936526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Bersihkan GPU memory\n",
    "del trainer\n",
    "del merged_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T06:37:57.507739Z",
     "iopub.status.busy": "2025-07-17T06:37:57.507208Z",
     "iopub.status.idle": "2025-07-17T06:37:57.512087Z",
     "shell.execute_reply": "2025-07-17T06:37:57.511435Z",
     "shell.execute_reply.started": "2025-07-17T06:37:57.507710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set logging back to normal\n",
    "logging.set_verbosity(logging.WARNING)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7843359,
     "sourceId": 12491413,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
